@ARTICLE{Cherubin2020-tz,
  title     = "{Tools for Reduced Precision Computation: A Survey}",
  author    = "Cherubin, Stefano and Agosta, Giovanni",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  53,
  number    =  2,
  pages     = "1--35",
  abstract  = "The use of reduced precision to improve performance metrics such
               as computation latency and power consumption is a common practice
               in the embedded systems field. This practice is emerging as a new
               trend in High Performance Computing (HPC), especially when new
               error-tolerant applications are considered. However, standard
               compiler frameworks do not support automated precision
               customization, and manual tuning and code transformation is the
               approach usually adopted in most domains. In recent years,
               research have been studying ways to improve the automation of
               this process. This article surveys this body of work, identifying
               the critical steps of this process, the most advanced tools
               available, and the open challenges in this research area. We
               conclude that, while several mature tools exist, there is still a
               gap to close, especially for tools based on static analysis
               rather than profiling, as well as for integration within
               mainstream, industry-strength compiler frameworks.",
  month     =  apr,
  year      =  2020,
  url       = "https://doi.org/10.1145/3381039",
  keywords  = "approximate computing, Reduced precision",
  doi       = "10.1145/3381039",
  issn      = "0360-0300"
}

@ARTICLE{McCormick2020-ic,
  title         = "{Algebraic error analysis for mixed-precision multigrid
                   solvers}",
  author        = "McCormick, Stephen F and Benzaken, Joseph and Tamstorf,
                   Rasmus",
  journal       = "arXiv [math.NA]",
  abstract      = "This paper establishes the first theoretical framework for
                   analyzing the rounding-error effects on multigrid methods
                   using mixed-precision iterative-refinement solvers. While
                   motivated by the sparse symmetric positive definite (SPD)
                   matrix equations that arise from discretizing linear elliptic
                   PDEs, the framework is purely algebraic such that it applies
                   to matrices that do not necessarily come from the continuum.
                   Based on the so-called energy or $A$ norm, which is the
                   natural norm for many problems involving SPD matrices, we
                   provide a normwise forward error analysis, and introduce the
                   notion of progressive precision for multigrid solvers. Each
                   level of the multigrid hierarchy uses three different
                   precisions that each increase with the fineness of the level,
                   but at different rates, thereby ensuring that the bulk of the
                   computation uses the lowest possible precision. The
                   theoretical results developed here in the energy norm differ
                   notably from previous theory based on the Euclidean norm in
                   important ways. In particular, we show that simply rounding
                   an exact result to finite precision causes an error in the
                   energy norm that is proportional to the square root of
                   $\kappa$, the associated matrix condition number. (By
                   contrast, this error is of order $1$ when measured in the
                   Euclidean norm.) Given this observation, we show that the
                   limiting accuracy for both V-cycles and full multigrid is
                   optimal in the sense that it is also proportional to
                   $\kappa^{1/2}$ in energy. Additionally, we show that the loss
                   of convergence rate due to rounding grows in proportion to
                   $\kappa^{1/2}$, but argue that this loss is insignificant in
                   practice. The theory presented here is the first forward
                   error analysis in the energy norm of iterative refinement and
                   the first rounding error analysis of multigrid in general.",
  month         =  jul,
  year          =  2020,
  url           = "http://arxiv.org/abs/2007.06614",
  archivePrefix = "arXiv",
  primaryClass  = "math.NA",
  eprint        = "2007.06614"
}

@ARTICLE{Tamstorf2021-ts,
  title     = "{Discretization-error-accurate mixed-precision multigrid solvers}",
  author    = "Tamstorf, Rasmus and Benzaken, Joseph and McCormick, Stephen F",
  journal   = "SIAM journal on scientific computing: a publication of the
               Society for Industrial and Applied Mathematics",
  publisher = "Society for Industrial \& Applied Mathematics (SIAM)",
  volume    =  43,
  number    =  5,
  pages     = "S420--S447",
  abstract  = "This paper builds on the algebraic theory in the companion paper
               [S. F. McCormick, J. Benzaken, and R. Tamstorf, SIAM J. Sci.
               Comput., (2021), pp. S392--S419] to obtain
               discretization-error-accurate solutions for linear elliptic
               partial differential equations (PDEs) by mixed-precision
               multigrid solvers. It is often assumed that the achievable
               accuracy is limited by discretization or algebraic errors. On the
               contrary, we show that the quantization error incurred by simply
               storing the matrix in any fixed precision quickly begins to
               dominate the total error as the discretization is refined. We
               extend the existing theory to account for these quantization
               errors and use the resulting bounds to guide the choice of four
               different precision levels in order to balance quantization,
               algebraic, and discretization errors in the progressive-precision
               scheme proposed in the companion paper. A remarkable result is
               that while iterative refinement is susceptible to quantization
               errors during the residual and update computations, the V-cycle
               used to compute the correction in each iteration is much more
               resilient and continues to work if the system matrices in the
               hierarchy become indefinite due to quantization. As a result, the
               V-cycle only requires relatively few bits of precision per level.
               Based on our findings, we outline a simple way to implement a
               progressive-precision full multigrid (FMG) solver with minimal
               overhead and demonstrate as an example that the one-dimensional
               biharmonic equation can be solved reliably to any desired
               accuracy using just a few V-cycles when the underlying smoother
               works well. Additionally, we show that the progressive-precision
               scheme leads to memory savings of up to 50\% compared to fixed
               precision.",
  month     =  jan,
  year      =  2021,
  url       = "https://epubs.siam.org/doi/10.1137/20M1349230",
  doi       = "10.1137/20m1349230",
  issn      = "1064-8275,1095-7197",
  language  = "en"
}

@ARTICLE{Vicuna2021-cr,
  title         = "{Reducing numerical precision preserves classification
                   accuracy in Mondrian Forests}",
  author        = "Vicuna, Marc and Khannouz, Martin and Kiar, Gregory and
                   Chatelain, Yohan and Glatard, Tristan",
  journal       = "arXiv [cs.LG]",
  abstract      = "Mondrian Forests are a powerful data stream classification
                   method, but their large memory footprint makes them
                   ill-suited for low-resource platforms such as connected
                   objects. We explored using reduced-precision floating-point
                   representations to lower memory consumption and evaluated its
                   effect on classification performance. We applied the Mondrian
                   Forest implementation provided by OrpailleCC, a C++
                   collection of data stream algorithms, to two canonical
                   datasets in human activity recognition: Recofit and Banos
                   \emph{et al}. Results show that the precision of
                   floating-point values used by tree nodes can be reduced from
                   64 bits to 8 bits with no significant difference in F1 score.
                   In some cases, reduced precision was shown to improve
                   classification performance, presumably due to its
                   regularization effect. We conclude that numerical precision
                   is a relevant hyperparameter in the Mondrian Forest, and that
                   commonly-used double precision values may not be necessary
                   for optimal performance. Future work will evaluate the
                   generalizability of these findings to other data stream
                   classifiers.",
  month         =  jun,
  year          =  2021,
  url           = "http://arxiv.org/abs/2106.14340",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2106.14340"
}

@MISC{vtune-profiler,
  title        = "{Intel\textregistered{} VTune\texttrademark{} Profiler Release
                  Notes and New Features}",
  author       = "{Intel}",
  booktitle    = "{Intel}",
  abstract     = "See the latest release notes available for
                  Intel\textregistered{} VTune\texttrademark{} Profiler
                  (including older versions starting with Intel\textregistered{}
                  VTune\texttrademark{} Amplifier XE 2017).",
  howpublished = "\url{https://www.intel.com/content/www/us/en/developer/articles/release-notes/vtune-profiler-release-notes.html}",
  note         = "Accessed: 2024-2-13",
  language     = "en"
}

@ARTICLE{Kurtzer2017-bu,
  title    = "{Singularity: Scientific containers for mobility of compute}",
  author   = "Kurtzer, Gregory M and Sochat, Vanessa and Bauer, Michael W",
  journal  = "PloS one",
  volume   =  12,
  number   =  5,
  pages    = "e0177459",
  abstract = "Here we present Singularity, software developed to bring
              containers and reproducibility to scientific computing. Using
              Singularity containers, developers can work in reproducible
              environments of their choosing and design, and these complete
              environments can easily be copied and executed on other platforms.
              Singularity is an open source initiative that harnesses the
              expertise of system and software engineers and researchers alike,
              and integrates seamlessly into common workflows for both of these
              groups. As its primary use case, Singularity brings mobility of
              computing to both users and HPC centers, providing a secure means
              to capture and distribute software and compute environments. This
              ability to create and deploy reproducible environments across
              these centers, a previously unmet need, makes Singularity a game
              changing development for computational science.",
  month    =  may,
  year     =  2017,
  url      = "http://dx.doi.org/10.1371/journal.pone.0177459",
  doi      = "10.1371/journal.pone.0177459",
  pmc      = "PMC5426675",
  pmid     =  28494014,
  issn     = "1932-6203",
  language = "en"
}

@MISC{ds004513:1.0.2,
  title     = "{The energetic costs of the human connectome}",
  author    = "Castrillon, Gabriel and Epp, Samira and Bose, Antonia and
               Fraticelli, Laura and Hechler, Andr\'{e} and Belenya, Roman and
               Ranft, Andreas and Yakushev, Igor and Utz, Lukas and Sundar,
               Lalith and Rauschecker, Josef P and Preibisch, Christine and
               Kurcyus, Katarzyna and Riedl, Valentin",
  publisher = "OpenNeuro",
  year      =  2023,
  url       = "https://openneuro.org/datasets/ds004513/versions/1.0.2",
  doi       = "10.18112/openneuro.ds004513.v1.0.2"
}

@MISC{noauthor_undated-iw,
  title       = "{docker2singularity: A docker image for converting docker
                 images to singularity images}",
  institution = "Github",
  abstract    = "A docker image for converting docker images to singularity
                 images. - singularityhub/docker2singularity",
  url         = "https://github.com/singularityhub/docker2singularity",
  language    = "en"
}

@ARTICLE{Yoo2002-ve,
  title    = "{Engineering and algorithm design for an image processing Api: a
              technical report on ITK--the Insight Toolkit}",
  author   = "Yoo, Terry S and Ackerman, Michael J and Lorensen, William E and
              Schroeder, Will and Chalana, Vikram and Aylward, Stephen and
              Metaxas, Dimitris and Whitaker, Ross",
  journal  = "Studies in health technology and informatics",
  volume   =  85,
  pages    = "586--592",
  abstract = "We present the detailed planning and execution of the Insight
              Toolkit (ITK), an application programmers interface (API) for the
              segmentation and registration of medical image data. This public
              resource has been developed through the NLM Visible Human Project,
              and is in beta test as an open-source software offering under
              cost-free licensing. The toolkit concentrates on 3D medical data
              segmentation and registration algorithms, multimodal and
              multiresolution capabilities, and portable platform independent
              support for Windows, Linux/Unix systems. This toolkit was built
              using current practices in software engineering. Specifically, we
              embraced the concept of generic programming during the development
              of these tools, working extensively with C++ templates and the
              freedom and flexibility they allow. Software development tools for
              distributed consortium-based code development have been created
              and are also publicly available. We discuss our assumptions,
              design decisions, and some lessons learned.",
  year     =  2002,
  url      = "https://www.ncbi.nlm.nih.gov/pubmed/15458157",
  pmid     =  15458157,
  issn     = "0926-9630",
  language = "en"
}

@ARTICLE{Avants2008-ea,
  title    = "{Symmetric diffeomorphic image registration with
              cross-correlation: evaluating automated labeling of elderly and
              neurodegenerative brain}",
  author   = "Avants, B B and Epstein, C L and Grossman, M and Gee, J C",
  journal  = "Medical image analysis",
  volume   =  12,
  number   =  1,
  pages    = "26--41",
  abstract = "One of the most challenging problems in modern neuroimaging is
              detailed characterization of neurodegeneration. Quantifying
              spatial and longitudinal atrophy patterns is an important
              component of this process. These spatiotemporal signals will aid
              in discriminating between related diseases, such as frontotemporal
              dementia (FTD) and Alzheimer's disease (AD), which manifest
              themselves in the same at-risk population. Here, we develop a
              novel symmetric image normalization method (SyN) for maximizing
              the cross-correlation within the space of diffeomorphic maps and
              provide the Euler-Lagrange equations necessary for this
              optimization. We then turn to a careful evaluation of our method.
              Our evaluation uses gold standard, human cortical segmentation to
              contrast SyN's performance with a related elastic method and with
              the standard ITK implementation of Thirion's Demons algorithm. The
              new method compares favorably with both approaches, in particular
              when the distance between the template brain and the target brain
              is large. We then report the correlation of volumes gained by
              algorithmic cortical labelings of FTD and control subjects with
              those gained by the manual rater. This comparison shows that, of
              the three methods tested, SyN's volume measurements are the most
              strongly correlated with volume measurements gained by expert
              labeling. This study indicates that SyN, with cross-correlation,
              is a reliable method for normalizing and making anatomical
              measurements in volumetric MRI of patients and at-risk elderly
              individuals.",
  month    =  feb,
  year     =  2008,
  url      = "http://dx.doi.org/10.1016/j.media.2007.06.004",
  doi      = "10.1016/j.media.2007.06.004",
  pmc      = "PMC2276735",
  pmid     =  17659998,
  issn     = "1361-8415,1361-8423",
  language = "en"
}

@ARTICLE{Avants2011-uk,
  title    = "{A reproducible evaluation of ANTs similarity metric performance
              in brain image registration}",
  author   = "Avants, Brian B and Tustison, Nicholas J and Song, Gang and Cook,
              Philip A and Klein, Arno and Gee, James C",
  journal  = "NeuroImage",
  volume   =  54,
  number   =  3,
  pages    = "2033--2044",
  abstract = "The United States National Institutes of Health (NIH) commit
              significant support to open-source data and software resources in
              order to foment reproducibility in the biomedical imaging
              sciences. Here, we report and evaluate a recent product of this
              commitment: Advanced Neuroimaging Tools (ANTs), which is
              approaching its 2.0 release. The ANTs open source software library
              consists of a suite of state-of-the-art image registration,
              segmentation and template building tools for quantitative
              morphometric analysis. In this work, we use ANTs to quantify, for
              the first time, the impact of similarity metrics on the affine and
              deformable components of a template-based normalization study. We
              detail the ANTs implementation of three similarity metrics:
              squared intensity difference, a new and faster cross-correlation,
              and voxel-wise mutual information. We then use two-fold
              cross-validation to compare their performance on openly available,
              manually labeled, T1-weighted MRI brain image data of 40 subjects
              (UCLA's LPBA40 dataset). We report evaluation results on cortical
              and whole brain labels for both the affine and deformable
              components of the registration. Results indicate that the best
              ANTs methods are competitive with existing brain extraction
              results (Jaccard=0.958) and cortical labeling approaches. Mutual
              information affine mapping combined with cross-correlation
              diffeomorphic mapping gave the best cortical labeling results
              (Jaccard=0.669$\pm{}$0.022). Furthermore, our two-fold
              cross-validation allows us to quantify the similarity of templates
              derived from different subgroups. Our open code, data and
              evaluation scripts set performance benchmark parameters for this
              state-of-the-art toolkit. This is the first study to use a
              consistent transformation framework to provide a reproducible
              evaluation of the isolated effect of the similarity metric on
              optimal template construction and brain labeling.",
  month    =  feb,
  year     =  2011,
  url      = "http://dx.doi.org/10.1016/j.neuroimage.2010.09.025",
  doi      = "10.1016/j.neuroimage.2010.09.025",
  pmc      = "PMC3065962",
  pmid     =  20851191,
  issn     = "1053-8119,1095-9572",
  language = "en"
}

@ARTICLE{Dugre2025-fs,
  title    = "{An analysis of performance bottlenecks in MRI preprocessing}",
  author   = "Dugr\'{e}, Mathieu and Chatelain, Yohan and Glatard, Tristan",
  journal  = "GigaScience",
  volume   =  24,
  pages    = "giae098",
  abstract = "Magnetic Resonance Image (MRI) pre-processing is a critical step
              for neuroimaging analysis. However, the computational cost of MRI
              pre-processing pipelines is a major bottleneck for large cohort
              studies and some clinical applications. While High-Performance
              Computing (HPC) and, more recently, Deep Learning have been
              adopted to accelerate the computations, these techniques require
              costly hardware and are not accessible to all researchers.
              Therefore, it is important to understand the performance
              bottlenecks of MRI pre-processing pipelines to improve their
              performance. Using Intel VTune profiler, we characterized the
              bottlenecks of several commonly used MRI-preprocessing pipelines
              from the ANTs, FSL, and FreeSurfer toolboxes. We found that few
              functions contributed to most of the CPU time, and that linear
              interpolation was the largest contributor. Data access was also a
              substantial bottleneck. We identified a bug in the ITK library
              that impacts the performance of ANTs pipeline in single-precision
              and a potential issue with the OpenMP scaling in FreeSurfer
              recon-all. Our results provide a reference for future efforts to
              optimize MRI pre-processing pipelines.",
  month    =  mar,
  year     =  2025,
  url      = "http://dx.doi.org/10.1093/gigascience/giae098",
  doi      = "10.1093/gigascience/giae098"
}

@ARTICLE{Grevera1998-dc,
  title    = "{An objective comparison of 3-D image interpolation methods}",
  author   = "Grevera, G J and Udupa, J K",
  journal  = "IEEE transactions on medical imaging",
  volume   =  17,
  number   =  4,
  pages    = "642--652",
  abstract = "To aid in the display, manipulation, and analysis of biomedical
              image data, they usually need to he converted to data of isotropic
              discretization through the process of interpolation. Traditional
              techniques consist of direct interpolation of the grey values.
              When user interaction is called for in image segmentation, as a
              consequence of these interpolation methods, the user needs to
              segment a much greater (typically 4-10x) amount of data. To
              mitigate this problem, a method called shape-based interpolation
              of binary data was developed 121. Besides significantly reducing
              user time, this method has been shown to provide more accurate
              results than grey-level interpolation. We proposed an approach for
              the interpolation of grey data of arbitrary dimensionality that
              generalized the shape-based method from binary to grey data. This
              method has characteristics similar to those of the binary
              shape-based method. In particular, we showed preliminary evidence
              that it produced more accurate results than conventional
              grey-level interpolation methods. In this paper, concentrating on
              the three-dimensional (3-D) interpolation problem, we compare
              statistically the accuracy of eight different methods:
              nearest-neighbor, linear grey-level, grey-level cubic spline,
              grey-level modified cubic spline, Goshtasby et al., and three
              methods from the grey-level shape-based class. A population of
              patient magnetic resonance and computed tomography images,
              corresponding to different parts of the human anatomy, coming from
              different three-dimensional imaging applications, are utilized for
              comparison. Each slice in these data sets is estimated by each
              interpolation method and compared to the original slice at the
              same location using three measures: mean-squared difference,
              number of sites of disagreement, and largest difference. The
              methods are statistically compared pairwise based on these
              measures. The shape-based methods statistically significantly
              outperformed all other methods in all measures in all applications
              considered here with a statistical relevance ranging from 10\% to
              32\% (mean = 15\%) for mean-squared difference.",
  month    =  aug,
  year     =  1998,
  url      = "http://dx.doi.org/10.1109/42.730408",
  doi      = "10.1109/42.730408",
  pmid     =  9845319,
  issn     = "0278-0062",
  language = "en"
}

@ARTICLE{Avants2020-xx,
  title     = "{Advanced normalization tools (ANTs)}",
  author    = "Avants, B and Tustison, N and Johnson, Hans J",
  journal   = "The insight journal",
  publisher = "psychiatry.ucsd.edu",
  abstract  = "... This update to ANTs documentation was initiated April 29,
               2014. This document does not cover all of ANTs functionality --
               but summarizes the most frequently used components ...",
  month     =  dec,
  year      =  2020,
  url       = "https://psychiatry.ucsd.edu/research/programs-centers/snl/_files/ants2.pdf",
  doi       = "10.5281/ZENODO.5138159"
}

@INPROCEEDINGS{Chen2018-ge,
  title     = "{Exploiting approximate computing for deep learning acceleration}",
  author    = "Chen, Chia-Yu and Choi, Jungwook and Gopalakrishnan, Kailash and
               Srinivasan, Viji and Venkataramani, Swagath",
  booktitle = "{2018 Design, Automation \& Test in Europe Conference \&
               Exhibition (DATE)}",
  publisher = "IEEE",
  pages     = "821--826",
  abstract  = "Deep Neural Networks (DNNs) have emerged as a powerful and
               versatile set of techniques to address challenging artificial
               intelligence (AI) problems. Applications in domains such as
               image/video processing, natural language processing, speech
               synthesis and recognition, genomics and many others have embraced
               deep learning as the foundational technique. DNNs achieve
               superior accuracy for these applications using very large models
               which require 100s of MBs of data storage, ExaOps of computation
               and high bandwidth for data movement. Despite advances in
               computing systems, training state-of-the-art DNNs on large
               datasets takes several days/weeks, directly limiting the pace of
               innovation and adoption. In this paper, we discuss how these
               challenges can be addressed via approximate computing. Based on
               our earlier studies demonstrating that DNNs are resilient to
               numerical errors from approximate computing, we present
               techniques to reduce communication overhead of distributed deep
               learning training via adaptive residual gradient compression
               (AdaComp), and computation cost for deep learning inference via
               Prameterized clipping ACTivation (PACT) based network
               quantization. Experimental evaluation demonstrates order of
               magnitude savings in communication overhead for training and
               computational cost for inference while not compromising
               application accuracy.",
  month     =  mar,
  year      =  2018,
  url       = "http://dx.doi.org/10.23919/DATE.2018.8342119",
  doi       = "10.23919/DATE.2018.8342119",
  isbn      = "9783981926309,9783981926316",
  issn      = "1558-1101"
}

@ARTICLE{Zuras2008-eg,
  title   = "{IEEE Standard for Floating-Point Arithmetic}",
  author  = "Zuras, D and Cowlishaw, M and Aiken, A and Applegate, Matthew and
             Bailey, D and Bass, Steve and Bhandarkar, D and Bhat, M and Bindel,
             D and Boldo, S and Canon, Stephen and Carlough, S and Cornea,
             Marius and Crawford, John and Darcy, Joseph D and Sarma, Debjit Das
             and Daumas, M and Davis, B and Davis, Mark and Delp, D and Demmel,
             J and Erle, Mark A and Fahmy, H and Fasano, J P and Fateman, R and
             Feng, Eric and Ferguson, W and Fit-Florea, A and Fournier, L and
             Freitag, Chip and Godard, Ivan and Golliver, Roger A and Gustafson,
             D and Hack, M and Harrison, J R and Hauser, J and Hida, Yozo and
             Hinds, C and Hoare, Graydon and Hough, David G and Huck, Jerry and
             Hull, J and Ingrassia, M and James, D and James, Rick and Kahan, W
             and Kapernick, J and Karpinski, Richard and Kidder, J and Koev, P
             and Li, Ren-Cang and Liu, Zhishun A and Mak, Raymond and Markstein,
             Peter W and Matula, D and Melquiond, G and Mori, Nobuyoshi and
             Morin, R and Nedialkov, N and Nelson, Craig E and Oberman, S and
             Okada, J and Ollmann, I and Parks, Michael and Pittman, Tom and
             Postpischil, Eric and Riedy, Jason and Schwarz, E and Scott, D and
             Senzig, D and Sharapov, I and Shearer, J and Siu, Michael and
             Smith, Ron and Stevens, C and Tang, Peter and Taylor, P and Thomas,
             James W and Thompson, Brandon and Thrash, W and Toda, N and Trong,
             S D and Tsai, L and Tsen, C and Tydeman, F and Wang, Liangyan and
             Westbrook, S and Winkler, S and Wood, A and Yalcinalp, Umit and
             Zemke, F and Zimmermann, P",
  journal = "IEEE Std",
  pages   = "1--70",
  year    =  2008,
  url     = "http://ieeexplore.ieee.org/document/4610935/",
  doi     = "10.1109/IEEESTD.2008.4610935"
}

@ARTICLE{Paszke2019-vk,
  title     = "{PyTorch: An imperative style, high-performance deep learning
               library}",
  author    = "Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam
               and Bradbury, James and Chanan, Gregory and Killeen, Trevor and
               Lin, Zeming and Gimelshein, N and Antiga, L and Desmaison, Alban
               and K{\"{o}}pf, Andreas and Yang, E and DeVito, Zach and Raison,
               Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner,
               Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith",
  journal   = "Advances in neural information processing systems",
  publisher = "proceedings.neurips.cc",
  volume    = "abs/1912.01703",
  abstract  = "Deep learning frameworks have often focused on either usability
               or speed, but not both. PyTorch is a machine learning library
               that shows that these two goals are in fact compatible: it was
               designed from first principles to support an imperative and
               Pythonic programming style that supports code as a model, makes
               debugging easy and is consistent with other popular scientific
               computing libraries, while remaining efficient and supporting
               hardware accelerators such as GPUs. In this paper, we detail the
               principles that drove the implementation of PyTorch and how they
               are reflected in its architecture. We emphasize that every aspect
               of PyTorch is a regular Python program under the full control of
               its user. We also explain how the careful and pragmatic
               implementation of the key components of its runtime enables them
               to work together to achieve compelling performance. We
               demonstrate the efficiency of individual subsystems, as well as
               the overall speed of PyTorch on several commonly used benchmarks.",
  month     =  dec,
  year      =  2019,
  url       = "https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html",
  eprint    = "1912.01703",
  issn      = "1049-5258"
}

@ARTICLE{Abadi2016-vm,
  title     = "{TensorFlow: Large-scale machine learning on heterogeneous
               distributed systems}",
  author    = "Abadi, Mart\'{\i}n and Agarwal, Ashish and Barham, P and Brevdo,
               E and Chen, Z and Citro, C and Corrado, G and Davis, Andy and
               Dean, J and Devin, M and Ghemawat, Sanjay and Goodfellow, I and
               Harp, A and Irving, G and Isard, M and Jia, Yangqing and
               J\'{o}zefowicz, R and Kaiser, Lukasz and Kudlur, M and Levenberg,
               J and Man\'{e}, Dandelion and Monga, R and Moore, Sherry and
               Murray, D and Olah, C and Schuster, M and Shlens, Jonathon and
               Steiner, Benoit and Sutskever, I and Talwar, Kunal and Tucker, P
               and Vanhoucke, Vincent and Vasudevan, Vijay and Vi\'{e}gas, F and
               Vinyals, O and Warden, Pete and Wattenberg, M and Wicke, M and
               Yu, Yuan and Zheng, Xiaoqiang",
  journal   = "ArXiv",
  publisher = "Mountain View, CA: Tensorflow",
  volume    = "abs/1603.04467",
  abstract  = "TensorFlow is an interface for expressing machine learning
               algorithms, and an implementation for executing such algorithms.
               A computation expressed using TensorFlow can be executed with
               little or no change on a wide variety of heterogeneous systems,
               ranging from mobile devices such as phones and tablets up to
               large-scale distributed systems of hundreds of machines and
               thousands of computational devices such as GPU cards. The system
               is flexible and can be used to express a wide variety of
               algorithms, including training and inference algorithms for deep
               neural network models, and it has been used for conducting
               research and for deploying machine learning systems into
               production across more than a dozen areas of computer science and
               other fields, including speech recognition, computer vision,
               robotics, information retrieval, natural language processing,
               geographic information extraction, and computational drug
               discovery. This paper describes the TensorFlow interface and an
               implementation of that interface that we have built at Google.
               The TensorFlow API and a reference implementation were released
               as an open-source package under the Apache 2.0 license in
               November, 2015 and are available at www.tensorflow.org.",
  month     =  mar,
  year      =  2016,
  url       = "http://arxiv.org/abs/1603.04467",
  eprint    = "1603.04467",
  issn      = "2331-8422"
}

@MISC{Wang2019-qe,
  title        = "{BFloat16: The secret to high performance on Cloud {TPUs}}",
  author       = "Wang, Shibo and Kanwar, Pankaj",
  booktitle    = "{Google Cloud Blog}",
  publisher    = "Google Cloud",
  abstract     = "How the high performance of Google Cloud TPUs is driven by
                  Brain Floating Point Format, or bfloat16",
  month        =  aug,
  year         =  2019,
  howpublished = "\url{https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus}",
  note         = "Accessed: 2024-5-3",
  language     = "en"
}

@INPROCEEDINGS{Chatelain2019-vw,
  title     = "{Automatic Exploration of Reduced Floating-Point Representations
               in Iterative Methods}",
  author    = "Chatelain, Yohan and Petit, Eric and de Oliveira Castro, Pablo
               and Lartigue, Ghislain and Defour, David",
  booktitle = "{Euro-Par 2019: Parallel Processing}",
  publisher = "Springer International Publishing",
  pages     = "481--494",
  abstract  = "With the ever-increasing need for computation of scientific
               applications, new application domains, and major energy
               constraints, the landscape of floating-point computation is
               changing. New floating-point representation formats are emerging
               and there is a need for tools to simulate their impact in legacy
               codes. In this paper, we propose an automatic tool to evaluate
               the effect of adapting the floating point precision for each
               operation over time, which is particularly useful in iterative
               schemes. We present a backend to emulate any IEEE-754
               floating-point operation in lower precision. We tested the
               numerical errors resilience of our solutions thanks to Monte
               Carlo Arithmetic and demonstrated the effectiveness of this
               methodology on YALES2, a large Combustion-CFD HPC code, by
               achieving 28\% to 67\% reduction in communication volume by
               lowering precision.",
  year      =  2019,
  url       = "http://dx.doi.org/10.1007/978-3-030-29400-7_34",
  doi       = "10.1007/978-3-030-29400-7\_34"
}

@ARTICLE{Denis2015-zf,
  title         = "{Verificarlo: checking floating point accuracy through Monte
                   Carlo Arithmetic}",
  author        = "Denis, Christophe and De Oliveira Castro, Pablo and Petit,
                   Eric",
  journal       = "arXiv [cs.MS]",
  abstract      = "Numerical accuracy of floating point computation is a well
                   studied topic which has not made its way to the end-user in
                   scientific computing. Yet, it has become a critical issue
                   with the recent requirements for code modernization to
                   harness new highly parallel hardware and perform higher
                   resolution computation. To democratize numerical accuracy
                   analysis, it is important to propose tools and methodologies
                   to study large use cases in a reliable and automatic way. In
                   this paper, we propose verificarlo, an extension to the LLVM
                   compiler to automatically use Monte Carlo Arithmetic in a
                   transparent way for the end-user. It supports all the major
                   languages including C, C++, and Fortran. Unlike
                   source-to-source approaches, our implementation captures the
                   influence of compiler optimizations on the numerical
                   accuracy. We illustrate how Monte Carlo Arithmetic using the
                   verificarlo tool outperforms the existing approaches on
                   various use cases and is a step toward automatic numerical
                   analysis.",
  month         =  sep,
  year          =  2015,
  url           = "http://arxiv.org/abs/1509.01347",
  archivePrefix = "arXiv",
  primaryClass  = "cs.MS",
  eprint        = "1509.01347"
}

@ARTICLE{Higham2019-xu,
  title     = "{Simulating Low Precision Floating-Point Arithmetic}",
  author    = "Higham, Nicholas J and Pranesh, Srikara",
  journal   = "SIAM Journal of Scientific Computing",
  publisher = "Society for Industrial and Applied Mathematics",
  volume    =  41,
  number    =  5,
  pages     = "C585--C602",
  abstract  = "The half-precision (fp16) floating-point format, defined in the
               2008 revision of the IEEE standard for floating-point arithmetic,
               and a more recently proposed half-precision format bfloat16, are
               increasingly available in GPUs and other accelerators. While the
               support for low precision arithmetic is mainly motivated by
               machine learning applications, general purpose numerical
               algorithms can benefit from it, too, gaining in speed, energy
               usage, and reduced communication costs. Since the appropriate
               hardware is not always available, and one may wish to experiment
               with new arithmetics not yet implemented in hardware, software
               simulations of low precision arithmetic are needed. We discuss
               how to simulate low precision arithmetic using arithmetic of
               higher precision. We examine the correctness of such simulations
               and explain via rounding error analysis why a natural method of
               simulation can provide results that are more accurate than actual
               computations at low precision. We provide a MATLAB function,
               chop, that can be used to efficiently simulate fp16, bfloat16,
               and other low precision arithmetics, with or without the
               representation of subnormal numbers and with the options of round
               to nearest, directed rounding, stochastic rounding, and random
               bit flips in the significand. We demonstrate the advantages of
               this approach over defining a new MATLAB class and overloading
               operators.",
  month     =  jan,
  year      =  2019,
  url       = "https://doi.org/10.1137/19M1251308",
  doi       = "10.1137/19M1251308",
  issn      = "1064-8275"
}

@INPROCEEDINGS{Zhang2019-ry,
  title     = "{QPyTorch: A Low-Precision Arithmetic Simulation Framework}",
  author    = "Zhang, Tianyi and Lin, Zhiqiu and Yang, Guandao and De Sa,
               Christopher",
  booktitle = "{2019 Fifth Workshop on Energy Efficient Machine Learning and
               Cognitive Computing - NeurIPS Edition (EMC2-NIPS)}",
  publisher = "IEEE",
  pages     = "10--13",
  abstract  = "Low-precision training reduces computational cost and produces
               efficient models. Recent research in developing new low-precision
               training algorithms often relies on simulation to empirically
               evaluate the statistical effects of quantization while avoiding
               the substantial overhead of building specific hardware. To
               support this empirical research, we introduce QPyTorch, a
               low-precision arithmetic simulation framework. Built natively in
               PyTorch, QPyTorch provides a convenient interface that minimizes
               the efforts needed to reliably convert existing codes to study
               low-precision training. QPyTorch is general, and supports a
               variety of combinations of precisions, number formats, and
               rounding options. Additionally, it leverages an efficient
               fused-kernel approach to reduce simulator overhead, which enables
               simulation of large-scale, realistic problems. QPyTorch is
               publicly available at https://github.com/Tiiiger/QPyTorch.",
  month     =  dec,
  year      =  2019,
  url       = "http://dx.doi.org/10.1109/EMC2-NIPS53020.2019.00010",
  doi       = "10.1109/EMC2-NIPS53020.2019.00010",
  isbn      = "9781665424189,9781665424196"
}

@ARTICLE{Yaniv2018-pv,
  title     = "{SimpleITK image-analysis Notebooks: A collaborative environment
               for education and reproducible research}",
  author    = "Yaniv, Ziv and Lowekamp, Bradley C and Johnson, Hans J and Beare,
               Richard",
  journal   = "Journal of digital imaging",
  publisher = "Springer",
  volume    =  31,
  number    =  3,
  pages     = "290--303",
  abstract  = "Modern scientific endeavors increasingly require team
               collaborations to construct and interpret complex computational
               workflows. This work describes an image-analysis environment that
               supports the use of computational tools that facilitate
               reproducible research and support scientists with varying levels
               of software development skills. The Jupyter notebook web
               application is the basis of an environment that enables flexible,
               well-documented, and reproducible workflows via literate
               programming. Image-analysis software development is made
               accessible to scientists with varying levels of programming
               experience via the use of the SimpleITK toolkit, a simplified
               interface to the Insight Segmentation and Registration Toolkit.
               Additional features of the development environment include user
               friendly data sharing using online data repositories and a
               testing framework that facilitates code maintenance. SimpleITK
               provides a large number of examples illustrating educational and
               research-oriented image analysis workflows for free download from
               GitHub under an Apache 2.0 license:
               github.com/InsightSoftwareConsortium/SimpleITK-Notebooks .",
  month     =  jun,
  year      =  2018,
  url       = "https://link.springer.com/article/10.1007/s10278-017-0037-8",
  keywords  = "Image analysis; Open-source software; Python; R; Registration;
               Segmentation",
  doi       = "10.1007/s10278-017-0037-8",
  pmc       = "PMC5959828",
  pmid      =  29181613,
  issn      = "0897-1889,1618-727X",
  language  = "en"
}

@ARTICLE{Hayford2024-kb,
  title     = {{Speeding up and reducing memory usage for scientific machine
               learning via mixed precision}},
  author    = {Hayford, Joel and Goldman-Wetzler, Jacob and Wang, Eric and Lu,
               Lu},
  journal   = {Computer methods in applied mechanics and engineering},
  publisher = {Elsevier BV},
  volume    = 428,
  number    = 117093,
  pages     = 117093,
  month     = aug,
  year      = 2024,
  url       = {https://www.sciencedirect.com/science/article/pii/S0045782524003499},
  doi       = {10.1016/j.cma.2024.117093},
  issn      = {0045-7825,1879-2138},
  language  = {en}
}


@ARTICLE{McCormick2014-ok,
  title     = "{ITK: enabling reproducible research and open science}",
  author    = "McCormick, Matthew and Liu, Xiaoxiao and Jomier, Julien and
               Marion, Charles and Ibanez, Luis",
  journal   = "Frontiers in neuroinformatics",
  publisher = "Frontiers Media SA",
  volume    =  8,
  pages     =  13,
  abstract  = "Reproducibility verification is essential to the practice of the
               scientific method. Researchers report their findings, which are
               strengthened as other independent groups in the scientific
               community share similar outcomes. In the many scientific fields
               where software has become a fundamental tool for capturing and
               analyzing data, this requirement of reproducibility implies that
               reliable and comprehensive software platforms and tools should be
               made available to the scientific community. The tools will
               empower them and the public to verify, through practice, the
               reproducibility of observations that are reported in the
               scientific literature. Medical image analysis is one of the
               fields in which the use of computational resources, both software
               and hardware, are an essential platform for performing
               experimental work. In this arena, the introduction of the Insight
               Toolkit (ITK) in 1999 has transformed the field and facilitates
               its progress by accelerating the rate at which algorithmic
               implementations are developed, tested, disseminated and improved.
               By building on the efficiency and quality of open source
               methodologies, ITK has provided the medical image community with
               an effective platform on which to build a daily workflow that
               incorporates the true scientific practices of reproducibility
               verification. This article describes the multiple tools,
               methodologies, and practices that the ITK community has adopted,
               refined, and followed during the past decade, in order to become
               one of the research communities with the most modern
               reproducibility verification infrastructure. For example, 207
               contributors have created over 2400 unit tests that provide over
               84\% code line test coverage. The Insight Journal, an open
               publication journal associated with the toolkit, has seen over
               360,000 publication downloads. The median normalized closeness
               centrality, a measure of knowledge flow, resulting from the
               distributed peer code review system was high, 0.46.",
  month     =  feb,
  year      =  2014,
  url       = "http://dx.doi.org/10.3389/fninf.2014.00013",
  keywords  = "ITK; code review; insight journal; insight toolkit; open science;
               reproducibility",
  doi       = "10.3389/fninf.2014.00013",
  pmc       = "PMC3929840",
  pmid      =  24600387,
  issn      = "1662-5196",
  language  = "en"
}

@ARTICLE{Zuo2014-ub,
  title     = "{An open science resource for establishing reliability and
               reproducibility in functional connectomics}",
  author    = "Zuo, Xi-Nian and Anderson, Jeffrey S and Bellec, Pierre and Birn,
               Rasmus M and Biswal, Bharat B and Blautzik, Janusch and Breitner,
               John C S and Buckner, Randy L and Calhoun, Vince D and
               Castellanos, F Xavier and Chen, Antao and Chen, Bing and Chen,
               Jiangtao and Chen, Xu and Colcombe, Stanley J and Courtney,
               William and Craddock, R Cameron and Di Martino, Adriana and Dong,
               Hao-Ming and Fu, Xiaolan and Gong, Qiyong and Gorgolewski,
               Krzysztof J and Han, Ying and He, Ye and He, Yong and Ho, Erica
               and Holmes, Avram and Hou, Xiao-Hui and Huckins, Jeremy and
               Jiang, Tianzi and Jiang, Yi and Kelley, William and Kelly, Clare
               and King, Margaret and LaConte, Stephen M and Lainhart, Janet E
               and Lei, Xu and Li, Hui-Jie and Li, Kaiming and Li, Kuncheng and
               Lin, Qixiang and Liu, Dongqiang and Liu, Jia and Liu, Xun and
               Liu, Yijun and Lu, Guangming and Lu, Jie and Luna, Beatriz and
               Luo, Jing and Lurie, Daniel and Mao, Ying and Margulies, Daniel S
               and Mayer, Andrew R and Meindl, Thomas and Meyerand, Mary E and
               Nan, Weizhi and Nielsen, Jared A and O'Connor, David and Paulsen,
               David and Prabhakaran, Vivek and Qi, Zhigang and Qiu, Jiang and
               Shao, Chunhong and Shehzad, Zarrar and Tang, Weijun and
               Villringer, Arno and Wang, Huiling and Wang, Kai and Wei, Dongtao
               and Wei, Gao-Xia and Weng, Xu-Chu and Wu, Xuehai and Xu, Ting and
               Yang, Ning and Yang, Zhi and Zang, Yu-Feng and Zhang, Lei and
               Zhang, Qinglin and Zhang, Zhe and Zhang, Zhiqiang and Zhao, Ke
               and Zhen, Zonglei and Zhou, Yuan and Zhu, Xing-Ting and Milham,
               Michael P",
  journal   = "Scientific data",
  publisher = "Springer Science and Business Media LLC",
  volume    =  1,
  number    =  1,
  pages     =  140049,
  abstract  = "Efforts to identify meaningful functional imaging-based
               biomarkers are limited by the ability to reliably characterize
               inter-individual differences in human brain function. Although a
               growing number of connectomics-based measures are reported to
               have moderate to high test-retest reliability, the variability in
               data acquisition, experimental designs, and analytic methods
               precludes the ability to generalize results. The Consortium for
               Reliability and Reproducibility (CoRR) is working to address this
               challenge and establish test-retest reliability as a minimum
               standard for methods development in functional connectomics.
               Specifically, CoRR has aggregated 1,629 typical individuals'
               resting state fMRI (rfMRI) data (5,093 rfMRI scans) from 18
               international sites, and is openly sharing them via the
               International Data-sharing Neuroimaging Initiative (INDI). To
               allow researchers to generate various estimates of reliability
               and reproducibility, a variety of data acquisition procedures and
               experimental designs are included. Similarly, to enable users to
               assess the impact of commonly encountered artifacts (for example,
               motion) on characterizations of inter-individual variation,
               datasets of varying quality are included.",
  month     =  dec,
  year      =  2014,
  url       = "http://dx.doi.org/10.1038/sdata.2014.49",
  doi       = "10.1038/sdata.2014.49",
  pmc       = "PMC4421932",
  pmid      =  25977800,
  issn      = "2052-4463,2052-4463",
  language  = "en"
}

@MISC{Google2024-pq,
  title        = "{bfloat16 representation compared to IEEE binary16 and
                  {binary32}}",
  author       = "{Google}",
  year         =  2024,
  howpublished = "\url{https://cloud.google.com/tpu/docs/images/bfloat16.png}",
  note         = "Accessed: 2024-5-3"
}
